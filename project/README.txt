 Semantic Perceptual Image Compression using Deep Convolution Networks

Our code is based on the code https://github.com/iamaaditya/image-compression-cnn, which is part of the paper [arxiv](https://arxiv.org/abs/1612.08712).

Instruction of using the code:

## Generating Map

    ```
    python generate_map.py <image_file>
    ```
Generates Map and overlay file inside 'output' directory.


## Compressing image using the Map

    ```
    python combine_images.py -image <image_file> -map <map_file>
    ```
Map file is the file generated by aforementioned step. Default name for map is `output/msroi_map.jpg`



## Training your own model

To train your model, you will need class labelled training examples, like CIFAR, Caltech or Imagenet.
There is no need for 'localization' ground truth.

1. Generate the data pickles
    ```
    python prepare_data.py
    ```
Make sure that self.images point to the directory containing images.

2. It is not required to use pretrained VGG weights, but if you do training will be faster. 
You may download pretrained weights referred in Params file as vgg_weights [from here](https://drive.google.com/file/d/0B5o40yxdA9PqOVI5dF9tN3NUc2c/view?usp=sharing).

3. Use train.py to train the model. Models will be saved in 'models' directory after every 10 epoch. All the parematers and hyper-paramter can be adjusted at param.py


## Evaluating metrics

1. Use the '-print_metrics' command while calling 'combine_images.py'. This will print the metrics on STDOUT with this format --

```
jpeg_psnr,jpeg_ssim,our_ssim,our_q,jpeg_psnrhvs,png_size,model_number,our_size,filename,jpeg_vifp,jpeg_q,jpeg_msssim,our_psnrhvsm,jpeg_psnrhvsm,our_vifp,our_psnr,our_msssim,our_psnrhvs,jpeg_size
```

2. Pass the file which contains one line of metrics (as shown above) to the file 'read_log.py'. This will print various stats, and also plot the graphs as shown in the paper.



# Credits

 * CNN structure based on VGG16, https://github.com/ry/tensorflow-vgg16/blob/master/vgg16.py
 * Channel independent feature maps (3D features) using https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#depthwise_conv2d_native 
 * GAP based on https://github.com/jazzsaxmafia/Weakly_detector/blob/master/src/detector.py
 * Conv2d layer based on https://github.com/carpedm20/DCGAN-tensorflow/blob/master/ops.py


